---
title: "HW2 STA521 Fall18"
author: 'Eduardo Coronado - ec243 - ecoronado92'
date: "Due September 23, 2018 5pm"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Backgound Reading

Readings: Chapters 3-4 in Weisberg Applied Linear Regression


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(GGally)
library(data.table)
library(dplyr)
library(tibble)
```


## Exploratory Data Analysis

```{r data, echo=FALSE, message=FALSE}
library(alr3)
data(UN3, package="alr3")
help(UN3) 
library(car)
```


\textbf{1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?}  
From the summary data below 6 out of the 7 variables have missing data, with `ModernC` and `Frate` being the ones with most NAs present. Also from the metadata we know that only 125 observations out of 210 have complete data for all variables.
```{r, echo=FALSE}
summary(UN3)
```
  
  
Additionally, \textbf{all variables are quantitative.}
```{r, echo=FALSE}
str(UN3, strict.width="cut")
```

\pagebreak
\textbf{2. What is the mean and standard deviation of each quantitative predictor?  Provide in a nicely formatted table}

```{r, echo=FALSE}
m = sapply(UN3, mean, na.rm=TRUE)
stdev = sapply(UN3, sd, na.rm=TRUE)
m_sd_mat = cbind(m,stdev)
kable(m_sd_mat, format = "markdown", col.names = c("Mean", "Standard Dev"), digits = 2, align = "c")
```


\textbf{3. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed based on your graphical EDA?}  

```{r Fig 1, fig.width=8, fig.height=5, echo=FALSE, warning=FALSE}
ggpairs(UN3[,2:7], progress=FALSE,
        upper = list(continuous = wrap("cor", size = 3) ),
        title ="Fig 1. Pairwise Comparisons of Quanitative Predictor Variables from UN3 Dataset") +
  theme_bw(base_size = 10) + 
  theme(axis.text = element_text(size = 5.5), plot.title = element_text(size =15))
```

Using the `ggpairs` function we can do an initial assessment of the relationships among the predictor values. From \textbf{Fig 1} above it is easy to notice at first glance that many relationships among predictor variables seem non-linear. The relationships between the `Change`, `Purban`, `Fertility`, and `Frate` variables seem to have somewhat of a linear relationship. The `Frate` variable does seems to have a non-multicolinear relationship with some of the variables as well. However, two predictors stand out from this plot - `PPgdp` and `Pop`. The `PPgdp`'s relationships seem to follow an increasing or decreasing exponential, while in `Pop` we can nottice what could be two potential outliers (\textbf{Fig2}, below). In this plot we can see that China and India seem as potential outliers in terms of population vs other entities in the dataset.

```{r Fig 2, echo=FALSE, fig.width=8}
setorderv(UN3,c("Pop"), order = -1, na.last = TRUE)
plot(UN3[,5]~UN3[,2], xlab="Annual Pop Growth Rate (% change)", ylab="Population, thousands")
with(UN3, text(UN3[1:2,5]~UN3[1:2,2], labels = row.names(UN3[1:2,]), pos = 2))
title("Fig 2. Annual Pop Growth Rate Change vs Population for 210 Countries")
```

Neverthless, it is important to note that the scale of `Pop` and `PPgdp` are several orders of magnitude higher compared to other predictors, which brings to mind possible linear transformations as a remedy. Thus, I explored whether a simple `log()` transformation would improve the linear relationship for some of the variables \textbf{Fig 1} (e.g. `logFertility`, `logPPgdp`, `logPop`). Even though this was a crude first transformation on the data, we can notice in \textbf{Fig 3} below that it does improve linear relationships among our predictors.

```{r Fig3, fig.width=8, fig.height=5, echo=FALSE, warning=FALSE}
UN3_log = UN3 %>% rownames_to_column() %>%
  mutate(logPPgdp = log(PPgdp), logPop = log(Pop), logFertility = log(Fertility)) %>%
  column_to_rownames("rowname")
ggpairs(UN3_log[,c(-1,-3,-5,-6)], progress = FALSE,
        upper = list(continuous = wrap("cor", size = 3) ), 
        title="Fig 3. Pairwise Comparisons of Transformed Predictor Variables from UN3 Dataset") +
  theme_bw(base_size = 10) + 
  theme(axis.text = element_text(size = 5.5), plot.title = element_text(size =15))
``` 
  
\pagebreak
Finally, we can see that a linear combination of the these variables could be helpful to predict `ModernC`, although some would require transformations and others wouldn't. However, it is important to note that some predictors exhibit multicolinearily which means adding them to the linear model would be redundant as these would be contributing to explain the same variance and we should consider as we continue to build and assess the fit of the model.
  
  
```{r Fig4, echo=FALSE, warning=FALSE, fig.width=20, fig.height=12, results="hide" }
#ggpairs(UN3[,c(-3,-5,-6)], progress = FALSE, 
 #       upper = list(continuous = wrap("cor", size = 10) ), 
  #      title="Fig 4. Pairwise Comparison of Six Predictor Variables and `ModernC` Response Variable from the UN3 Dataset") + 
  #theme_bw(base_size = 25) + 
  #theme(axis.text = element_text(size = 12), plot.title = element_text(size = 25))
```


\textbf{4.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot from the linear model object and comment on results regarding assumptions.  How many observations are used in your model fitting?}  
  
From from the initial comparisons in \textbf{Fig3} we know that the predictor variables have a linear relationship which allows us to interpret these multiple regression diagnostic plots as those of a simple regression model. \textit{\textbf{Note}: the above mentioned model and following diagnostic plots were done using the original, non-transformed data}  
  
Using the `summary` function, we notice that the `lm` function automatically excluded 85 observations. Therefore, the multiple regression model was done 125 observations. Looking at \textbf{Fig 5} we can notice a minor heteroscedastic trend on the fitted vs residual plot, which shows that the variances is non-constant. However, this is a slight trend and doesn't have much effect on our assumptions of normality still hold. On the Normal Q-Q plot we observe some points diverging from the normal line - especially on the top-right - which means our data follows a skewd normal distribution. Yet, our assumptions still hold as the observed standard deviations seem to follow the theortical ones. Similarly, these plots show few standardize errors above 1 standard deviation and also few observations with high leverage that could be influencing the fit (i.e. China and India). However, these high leverage points  aren't significant enough to discard our assumptions  (i.e. Cook's Distance $<0.5$). Overall, Cook Islands, Azerbaijian, Poland, China, and India are candidates for outlier testing given these observations tend to be farther away from the rest of the data or are potentially influencial.  

\pagebreak
\textit{\textbf{Fig 5. Residual Plots for a Linear Model Fit of `ModernC`}}  

```{r, echo=FALSE, fig.width=8, fig.height=6}
modern_lm = lm(ModernC ~ . , data = UN3[,-8:-10])
#summary(modern_lm)
par(mfrow=c(2,2))
plot(modern_lm, ask=F)
```
  
  
\textbf{5. Examine added variable plots `car::avPlot` or `car::avPlots`  for your model above. Are there any plots that suggest that transformations are needed for any of the terms in the model? Describe. Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms?}  
  
From the plots in \textbf{Fig 6} we can observe that a transformation on `Pop` would be helpful. This is noticeble by the large amount of data points concentrated near zero while two points - China and India - are orders of magnitude apart. Adding a linear transformation, such as `log`, would help to reduce the skewedness of the plot by bringing the values near zero and far away from zero closer together. It could also help reduce the possible influence of the China/India observations on the fit of the model.  
It is also noticeable how certain countries are influential for specific terms. For example, Kuwait and Cook's Island are an influential point for `Change`. India and China, as seen before, are influential on the `Pop` term. Noticeably, these plots brought up new influential localities for specific terms such as Norway and Switzerland for `PPgdp`, Niger and Thailand for `Fertility`, Yemen and Burundi for `Frate`, and Sri Lanka and Thailand for `Purban`. Again, we can notice previous localities such as Poland, Azerbaijian, and Cook's Island as being influential for certain terms such as `PPgdp` or `Purban`, among others.  
Overall, from these plots we can notice the explanatory power of each predictor on the response variable after accounting for all the other predictors. From the slope we can notice almost all terms have either a positive or negative linear relationship with the response variable and they contribute to explain the variability. `PUrban` and `Frate` seem to have the least explanatory power after accounting for all other predictors. A possible cause for this would be an existing multi-colinear relationship with another term already accounted in the linear model.  
  
```{r Fig 6, echo=FALSE, fig.height=7}
avPlots(model = modern_lm, main="Fig 6. Added-Variable Plots for 6 Predictors of `modern_lm` Model")
```

\textbf{6.  Using the Box-Tidwell  `car::boxTidwell` or graphical methods find appropriate transformations of the predictor variables to be used as predictors in the linear model.  If any predictors are negative, you may need to transform so that they are non-negative.  Describe your method and the resulting transformations.} 
  
As mentioned in the previous question, `Pop` seemed as the most promising candidate that would benefit from a transformation given the data was clustered near zero with two points spread far away. Using the `boxTidwell` built-in function the optimal transformation is close to $\lambda =0.5$ (i.e $\sqrt{Pop}$). However, from this method the transformation does not significantly improve the fit as shown by a non-significant p-value with $\alpha = 0.5$ (i.e. the transformation isn't significantly different from $\lambda = 1$ - our $H_0$. I similarly tested other predictors, but the suggested transformations  didn't provide enough evidence to reject the $H_0$. Just to double check that highly influential points on `Pop` such as China and India weren't giving us a different transform estimation, I removed them from a modified dataframe and re-test. Even when we remove these we still fail to reject $H_0$.  
However, graphically from \textbf{Fig 3} we can see that a `log()` transformation does improve the linear relationship between the `Pop`, `PPgdp` and `ModernC` - thus I will use these transformations.

```{r, echo=FALSE, warning=FALSE, results='hide'}
UN3_bxtdw = na.omit(UN3)
UN3_bxtdw["Change"] = UN3_bxtdw["Change"] + 2
boxTidwell(ModernC ~  Pop + PPgdp  + Frate, other.x = ~ Change + Fertility + Purban, data=UN3_bxtdw)
boxTidwell(ModernC ~  Purban  , other.x = ~ Change + PPgdp + Pop +Frate + Fertility , data=UN3_bxtdw)

#UN3_no_Ch_In = UN3_bxtdw %>% rownames_to_column() %>% filter(!rownames(UN3_bxtdw) %in% c("China", "India")) %>% column_to_rownames("rowname")
#UN3_less_lm = lm(ModernC ~ ., data = UN3_no_Ch_In)
#boxTidwell(ModernC ~ Pop, other.x = ~ Change + PPgdp + Purban +Frate + Fertility , data=UN3_no_Ch_In)
#avPlots(UN3_less_lm)
```


\textbf{7. Given the selected transformations of the predictors, select a transformation of the response using `MASS::boxcox` or `car::boxCox` and justify.}  
  
From the plot below we can observe the MLE for $\lambda$ using `boxCox`. Since the 95% confidence interval doesn't include 1 and the `boxCox` $\lambda = 0.76$, I decided to use the closest $\lambda$ that would still allow for some interpretation $\lambda = 0.5$. This is because we want a model that is both improved by a transformation, but at the same time it is interpretable.


```{r boxCox, echo=FALSE, fig.width=8, fig.height=5}
modern_lm2 = lm(ModernC ~ ., data = UN3_log[,c(-3,-5,-10)]) #lm w transformed PPgdp,Pop
#summary(modern_prlog_lm)
boxCox(modern_lm2)
lambda_y = powerTransform(modern_lm2)$lambda
```

\pagebreak
\textbf{8.  Fit the regression using the transformed variables.  Provide residual plots and added variables plots and comment.  If you feel that you need additional transformations of either the response or predictors, repeat any steps until you feel satisfied.}  
  
From the previous question I had decided to use $\lambda = 0.5$ for the response variable given it was close to the significant MLE for this parameter with $\alpha = 0.50$. However, after plotting the residual and added variable plots I noticed this transformation wasn't benefitial. After trying the $\lambda$'s MLE and $\lambda=1$ as well, I decided to proceed with the later given it improves the model and interpretability. Thus in \textbf{Fig 7} we observe the residual plots for,
$$ModernC = \beta_0 + \beta_1Change + \beta_2logPPgdp + \beta_3Frate + \beta_4logPop + \beta_5Frate + \beta_6Purban$$
Compared to \textbf{Fig 5}, we can observe two improvements: 1) we don't see the previous high leverage points (China/India) near a Cook's Distance that would raise a red flag for influential points nor any other point, 2) there is a minor correction of the points in both tails toward the diagonal line in the Normal Q-Q. Even when the transformations did very slight corrections to the minor heteroscedastic trends, these are still within reasonable bounds.

\textit{\textbf{Fig 7. Residual Plots for a Transformed Linear Model Fit of `ModernC`}}  
```{r Q8 part 1, echo=FALSE, fig.width=8, fig.height=6}
modern_lm2 = lm(ModernC ~ Change + logPPgdp + Frate + logPop + Fertility + Purban, data = UN3_log[,c(-3,-5,-10)])
#summary(modern_lm2)
par(mfrow=c(2,2))
plot(modern_lm2, ask=F)
```
  
\pagebreak  
Comparing the added-variable plots to \textbf{Fig 6}, we can see that in \textbf{Fig 8} there are two changes on the transformed predictors. `logPop`'s slope is less prominent than without a transformation  now that we have China or India closer to the other observations and not exerting an higher influence on the value of the coefficient. In `lopPPgdp` the data is closer to each other as expected with the transformation and we don't see Norway or Switzerland as potential outliers as we did before.

```{r Q8 part 2, echo=FALSE, fig.width=8, fig.height=6}
avPlots(model = modern_lm2, main="Fig 8. Added-Variable Plots for 6 Predictors of `modern_lm2` Model")
```
\textbf{9. Start by finding the best transformation of the response and then find transformations of the predictors.  Do you end up with a different model than in 8?}  
  
Yes we would have a slightly different model if we start with a transformation for the response that is close to the `boxCox` MLE of $\lambda=0.78$  and that is also interpretable (i.e. $\lambda = 0.5$, see plot below). Assuming we proceed with this value, we would find a `boxTidwell`  $\lambda$ for `Fertility` that is significantly different ($\alpha = 0.05$)  from 1 - i.e. $\lambda = 1.5$. To make it more interpretable I would have chosen $\lambda = 2$ instead for `Fertility`. However, given the 95% confidence interval for the response $\lambda$ includes 1 (as seen below), I wouldn't feel comfortable choosing any transformation in the first place. Thus, we would be back looking to find predictor $\lambda s$ with an untransformed response which was shown in Question 6.

```{r, echo=FALSE, warning=FALSE, results='hide', fig.width=8, fig.height=5}
boxCox(modern_lm)
powerTransform(modern_lm)
boxTidwell(sqrt(ModernC) ~ Pop + PPgdp + Fertility, other.x = ~ Change + Frate + Purban, data = UN3_bxtdw)
```
  
  
\textbf{10.  Are there any outliers or influential points in the data?  Explain.  If so, refit the model after removing any outliers and comment on residual plots.}  
  
I used the Bonferroni Correction to test for outliers where $p_i < \frac{\alpha}{n}$ where $\alpha = 0.5$. From this test no observations have signficant p-value [results and code hidden], thus none can be discarded as outliers. Regarding influential points, we can observe from \textbf{Fig 7} that even though with the transformations there are points with high leverage such as `Cook Islands`, `Vanuatu` and `Kuwait`. However, none are influential points.

```{r, results='hide', echo=FALSE }
pval = 2*(1 - pt(abs(rstudent(modern_lm2)), modern_lm2$df -1))
rownames(na.omit(UN3_log))[pval < .05/nrow(na.omit(UN3_log))]
```

## Summary of Results

11. For your final model, provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations of each coefficient.  These should be in terms of the original units!  
Before finalizing the model and deciding which predictors to include, I tested whether keeping `Purban` would improve the model and reduce the RSS. To do so I used an ANOVA to test $H_0$ - i.e. whether $\beta_6 = 0$ or not. From the resulting p-value (with $\alpha = 0.05$), we fail to reject $H_0$ and therefore I have excluded `Purban` from the final model:

$$Final\; Model \rightarrow \quad ModernC = \beta_0 + \beta_1Change + \beta_2logPPgdp + \beta_3Frate + \beta_4logPop + \beta_5Fertility $$

```{r, echo=FALSE}
modern_lm2_noPurban = lm(ModernC ~ Change + logPPgdp + Frate + logPop + Fertility, data = UN3_log)
anova(modern_lm2_noPurban, modern_lm2)
```
  
\pagebreak 
Below is summary table of the coefficients with 95% confidence interval in terms of the original units.
```{r, echo=FALSE}
coef_df = as.data.frame(confint(modern_lm2_noPurban))
coef_df["logPop",] = exp(coef_df["logPop",])
coef_df["logPPgdp",] = exp(coef_df["logPPgdp",])
rownames(coef_df) = rownames(confint(modern_lm))[-7] #Removing Purban
kable(coef_df, format = "markdown", col.names = c(paste("2.5%"), paste("97.5%")), digits = 3)
```


12. Provide a paragraph summarizing your final model  and findings suitable for the US envoy to the UN after adjusting for outliers or influential points.   You should provide a justification for any case deletions in your final model  

The following model provides a linear relationship between the percent of unmarried women using a modern method of contraception (`ModernC`) and a combination of socio-economic indicators (`Change`, `PPgdp`, `Frate`, `Pop`, `Fertility`, `Purban`) from 125 entities. 

$$ModernC = 4.10 + 4.70(Change) + 4.85(logPPgdp) + 0.20(Frate) + 1.44(logPop) - 9.27(Fertility) $$
  
This model can be interpreted in the following manner,  
1. Every unit increase in annual population growth rate implies a 4.70 unit increase in percent of unmarried women using contraception (with all else held constant)  
2. Similar claims can be done for `Frate` and `Fertility`. A 5 unit increase in `Frate` implies a  $0.2$ unit increase in `ModernC`, while a 1 unit increase in `Fertility` implies a $-9.27$ decrease in `ModernC`. For each case we must keep the others indicators constant.  
3. For `Pop`, we can interpret it using percent changes in `Pop`.  For example, every doubling of the population (i.e. 100% increase) implies an increase of $0.43$ units in `ModernC` with every other indicator help constant.  
4. Similarly, for `PPgdp` when the per capita GDP (in USD) is doubled (i.e. 100% increase) this implies an increase of $1.46$ units in `ModernC` with all else held constant.    
  
Although the original data contained data from 210 entities, 85 entities were excluded due to missing data in one or more socio-economic indicator.



## Methodology

    
13. Prove that the intercept in the added variable scatter plot will always be zero.  _Hint:  use the fact that if $H$ is the project matrix which contains a column of ones, then $1_n^T (I - H) = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept.  
\textbf{Before starting some clarifications on the notation:}
Let's assume our model is,
\begin{align*}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2
\end{align*}
* $\hat Y_{(1)}$ refers to the Y vector regressed on all the predictors minus the $X_1$ predictor
* $\hat X_{1 \cdot 2}$ refers the $X_1$ predictor being regressed on the rest of the predictors (i.e. $X_2$)
* Therefore, $X_1 - \hat X_{1 \cdot 2} = \hat e_{1 \cdot 2}$ refers to the residuals from the regression of predictor $X_1$ on the $X_2$.
* Similarly, $Y - \hat Y_{(1)} = \hat e_{(1)}$ refers to the residuals from the regression of Y on all the predictors minus the $X_1$
* Althought the correct notation for the hat matrix without $X_1$ would be $H_{(1)}$, we will use $H$ for simplicity given both regressions use the same hat matrix
  
Knowing the above we can prove that $\beta_0$ of an added variable plot will always be zero from,
\begin{gather*}
\hat e_{(1)} =  \vec{1}\hat \beta_0 + \hat \beta_1 \hat e_{1 \cdot 2}
\end{gather*}

Thus we can find $\hat \beta_1$ using the $(X^T X)^{-1} X^T Y$ notation, but now thinking of $X \equiv \hat e_{1 \cdot 2}$ and $Y \equiv \hat e_{(1)}$.
\begin{gather*}
\hat e_{(1)} =  \vec{1}\hat \beta_0 + \overbrace{\bigg[ \big((I-H)X_{1}\big) ^T (I-H)X_{1}\bigg]^{-1} \bigg[(I-H)X_{1}\bigg]^T Y}^{\hat \beta_1} \;(I-H)X_1 \\[1ex]
= \vec{1}\hat \beta_0 + \bigg[ X_{1}^T \underbrace{(I-H)(I-H)}_{ (I-H)}X_{1}\bigg]^{-1} \underbrace{X_{1}^T(I-H)Y}_{1x1\: scalar} \underbrace{(I-H)X_1}_{1x1\: scalar}
\end{gather*}

If you multiply both sides by $X_{1}^T$ and rearrange the scalars you get,
\begin{gather*}
X_{1}^T (I-H) Y = X_{1}^T \vec{1}\hat \beta_0 +  \underbrace{X_{1}^T (I-H)X_1\big[ X_{1}^T (I-H) X_{1}\big]^{-1}}_{I} X_{1}^T (I-H)Y \\[1ex]
\therefore \qquad X_{1}^T (I-H) Y = \sum_{i=1}^n X_{i,1} \hat \beta_0 + X_{1}^T (I-H) Y\\[1ex]
\sum_{i=1}^n X_{i,1} \hat \beta_0 = X_{1}^T (I-H) Y - X_{1}^T (I-H) Y = 0
\end{gather*}

Thus we can see that the only way this relationship can only be zero is if $\hat \beta_0 = 0$ (i.e. the intercept is $0$).

14. For multiple regression with more than 2 predictors, say a full model given by `Y ~ X1 + X2 + ... Xp`   we create the added variable plot for variable `j` by regressing `Y` on all of the `X`'s except `Xj` to form `e_Y` and then regressing `Xj` on all of the other X's to form `e_X`.  Confirm that the slope in a manually constructed added variable plot for one of the predictors  in Ex. 10 is the same as the estimate from your model.  
  
To confirm that the coefficients are the same, I obtained the residuals from regressing `ModernC` on all predictors except `Purban` (e_Y) and the residuals from regressing `Purban` on the rest of the predictors (e_Purban). Then, regressing these two linear models provides us with the coefficient for `Purban` when compared to that of the original model (i.e. Y regressed on all predictors). \textbf{Fig 9} below shows the plot e_Y vs. e_Purban and demonstrates a slope of $-0.070768$

```{r , echo=FALSE, fig.width=8}
e_Y = residuals(modern_lm2_noPurban)
e_Purban = residuals(lm(Purban ~ Change + logPPgdp + Frate + 
                          logPop + Fertility, data = na.omit(UN3_log)))

e_Y_Purban = lm(e_Y ~ e_Purban)

df = data.frame(Original_coef = modern_lm2$coefficients["Purban"],
                avPlot_coef = e_Y_Purban$coefficients["e_Purban"], row.names = "Coeffs")

kable(df, format="markdown", col.names = c("Original", " A-V Plot"))

df = data.frame(e_Y = e_Y, e_X1 = e_Purban)
ggplot(data=df, aes(x = e_Purban, y = e_Y)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + ggtitle("Fig 9. Added-Variable Plot for `ModernC` ~ X w/o `Purban` vs `Purban` ~ X w/o `Purban`")
```
